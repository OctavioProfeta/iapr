{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Lab 3 ‒  Classification\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 19.04.2023  \n",
    "**Due date:** 05.05.2023 \n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class well as\n",
    "preparation for the final project, which is a practical project which ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like opencv and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "We will not rerun the notebook for you.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use PyTorch. If you are not familiar with this library, [here](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html) is a quick tutorial of the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darwin\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==1.8.1 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==1.8.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# import platform\n",
    "# print(platform.system())\n",
    "# if platform.system() == \"Darwin\":\n",
    "#     %pip install torch==1.12.1 torchvision==0.13.1\n",
    "# else:\n",
    "#     %pip install torch==1.8.1+cu111 torchvision==0.9.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "data_base_path = os.path.join(os.pardir, 'data')\n",
    "data_folder = 'lab-03-data'\n",
    "tar_path = os.path.join(data_base_path, data_folder + '.tar.gz')\n",
    "with tarfile.open(tar_path, mode='r:gz') as tar:\n",
    "    tar.extractall(path=data_base_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 - Out-of-Distribution detection in colorectal cancer histology (12 points)\n",
    "\n",
    "Colorectal cancer is one of the most widespread cancers for men and women. Diagnosis complemented with prognostic and predictive biomarker information is essential for patient monitoring and applying personalized treatments. A critical marker is the tumor/stroma ratio in unhealthy tissues sampled from the colon. The higher the ratio, the more invasive the cancer is. The degree of invasion is tightly linked to patient survial probability.\n",
    "\n",
    "To measure the ratio, a pathologist needs to analyze the unhealthy tissue under a microscope and estimate it from a look. As the number of samples to analyze is huge and estimations are only sometimes precise, automatic recognition of the different tissue types in histological images has become essential. Such an automatic process requires the development of a multi-class classifier to identify the numerous tissues. As shown below, they are usually 8 tissue types to categorize: TUMOR, STROMA, LYMPHO (lymphocytes), MUCOSA, COMPLEX (complex stroma), DEBRIS, ADIPOSE and EMPTY (background).\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<figure>\n",
    "    <img src=\"../data/lab-03-data/part1/kather16.svg\" width=\"1100\">\n",
    "    <center>\n",
    "    <figcaption>Fig1: Collection of tissue types in colorectal cancer histology (Kather-16)</figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "\n",
    "Up to this day, state-of-the-art methods use deep-learning-based supervised learning methods. A downfall of such an approach is the necessity to access a well-annotated training dataset. In histology, annotating data is difficult. It is time-consuming and requires the expertise of pathologists. Moreover, the annotator must label every tissue type while only two (TUMOR and STROMA) are interesting. \n",
    "\n",
    "\n",
    "Consequently, we propose another approach. In order to make the annotation task less tedious, we ask the annotator to label only the tissues of interest and dump the others. Then, we must train a binary classifier to automatically recognize these tissues at test time. In this part, you will implement the proposed approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Binary classifier with Mahalanobis distance (3 points)\n",
    "\n",
    "Based on the abovementioned process, your task is to build a model that recognizes TUMOR (Label 0) and STROMA (Label 1) tissue types. Your model will be supervised by a training dataset containing TUMOR and STROMA annotations; note that all other tissues have been dropped.\n",
    "We will not ask you to train a deep-learning-based binary classifier from scratch. Instead, we provide excellent features (descriptors) of the images we extracted from a visual foundation model. (Note: As the nature of the foundation model is not part of this lecture, feel free to ask TAs if you are curious).\n",
    "\n",
    "Run the cell below to extract the provided train and test dataset. Each image is represented by a 768-d feature vector extracted from a visual foundation model. The train and test datasets contain feature vectors of 878 and 186 images respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([186, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "# Label mapping\n",
    "label_to_classname = {0 : \"TUMOR\", 1 : \"STROMA\"}\n",
    "\n",
    "# Train features and labels\n",
    "train_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_features.pth\"))\n",
    "train_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_train_labels.pth\"))\n",
    "\n",
    "# Test features and labels\n",
    "test_features = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_features.pth\"))\n",
    "test_labels = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test_labels.pth\"))\n",
    "\n",
    "test_features.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (2.5 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using minimum Mahalanobis distance.\n",
    "\n",
    "*Note:* You are not allowed to use any prebuilt Mahalanobis distance function. Additionally, ```torch.cov``` is not defined to compute the covariance matrix. You can use ```sklearn.covariance.LedoitWolf``` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(17.1802), tensor(17.6804), tensor(17.5610), tensor(19.5801), tensor(19.7986), tensor(18.4980), tensor(17.7227), tensor(18.0925), tensor(19.3335), tensor(18.9395), tensor(18.5379), tensor(17.6532), tensor(18.5126), tensor(18.5117), tensor(18.3458), tensor(18.3620), tensor(19.6553), tensor(18.2747), tensor(19.5748), tensor(18.3150), tensor(18.3295), tensor(18.6685), tensor(19.6575), tensor(18.6608), tensor(17.2290), tensor(17.3864), tensor(18.1821), tensor(18.9504), tensor(18.5134), tensor(16.9436), tensor(17.8227), tensor(19.4465), tensor(16.9520), tensor(16.9989), tensor(17.6098), tensor(17.4415), tensor(20.1831), tensor(16.9711), tensor(17.3108), tensor(17.3501), tensor(18.2577), tensor(18.7261), tensor(18.4225), tensor(17.0939), tensor(17.1903), tensor(17.6368), tensor(17.5700), tensor(17.5407), tensor(18.4585), tensor(17.9064), tensor(18.6782), tensor(18.4919), tensor(17.6186), tensor(18.3135), tensor(17.9202), tensor(19.3169), tensor(16.0368), tensor(18.9719), tensor(17.8351), tensor(18.9218), tensor(18.6306), tensor(18.4012), tensor(18.8286), tensor(16.7075), tensor(18.7753), tensor(18.1114), tensor(18.5833), tensor(19.5044), tensor(18.9278), tensor(18.0596), tensor(19.2150), tensor(19.0245), tensor(17.5218), tensor(19.2272), tensor(18.2467), tensor(17.7937), tensor(17.4275), tensor(19.2911), tensor(18.7529), tensor(17.1776), tensor(17.6284), tensor(19.9778), tensor(18.4624), tensor(17.3666), tensor(19.4375), tensor(19.3604), tensor(19.2335), tensor(17.6696), tensor(17.0972), tensor(19.2932), tensor(18.5226), tensor(15.7519), tensor(18.0909), tensor(19.1538), tensor(17.9943), tensor(19.4110), tensor(17.3656), tensor(18.6267), tensor(19.4458), tensor(19.0433), tensor(19.2510), tensor(19.6629), tensor(18.7531), tensor(19.0811), tensor(18.5255), tensor(17.3345), tensor(18.4184), tensor(19.3425), tensor(17.2945), tensor(18.5045), tensor(18.7952), tensor(17.2165), tensor(18.6858), tensor(19.5710), tensor(17.2973), tensor(17.8567), tensor(17.7258), tensor(17.3188), tensor(18.8879), tensor(16.1063), tensor(18.4279), tensor(20.6047), tensor(16.3060), tensor(19.3979), tensor(19.4934), tensor(19.0519), tensor(18.5276), tensor(19.3218), tensor(18.6507), tensor(17.3211), tensor(18.4024), tensor(17.5945), tensor(18.3342), tensor(17.5937), tensor(17.3371), tensor(17.7998), tensor(18.3134), tensor(19.4143), tensor(19.2633), tensor(18.6164), tensor(15.0123), tensor(18.5896), tensor(18.7124), tensor(18.3874), tensor(18.8402), tensor(19.1555), tensor(16.4827), tensor(18.3083), tensor(19.0937), tensor(19.2174), tensor(18.7888), tensor(18.2712), tensor(18.7245), tensor(19.0645), tensor(20.0086), tensor(17.3432), tensor(17.4941), tensor(19.0683), tensor(18.3260), tensor(18.4515), tensor(19.4812), tensor(20.6602), tensor(19.1577), tensor(19.0420), tensor(18.4936), tensor(19.3566), tensor(17.5753), tensor(19.2169), tensor(17.7839), tensor(17.6095), tensor(18.4383), tensor(18.1654), tensor(19.5209), tensor(16.0714), tensor(18.8956), tensor(18.8948), tensor(19.2513), tensor(16.6354), tensor(17.2511), tensor(16.6086), tensor(19.2810), tensor(17.2696), tensor(18.7444), tensor(20.0807), tensor(17.4106), tensor(16.2875), tensor(18.1515), tensor(16.8869), tensor(18.4751), tensor(17.0056), tensor(17.5611), tensor(18.0629), tensor(18.0364), tensor(18.5378), tensor(17.9623), tensor(16.9915), tensor(18.7709), tensor(19.7981), tensor(19.2708), tensor(18.2781), tensor(17.8209), tensor(18.6216), tensor(18.1258), tensor(15.7589), tensor(17.2588), tensor(19.5261), tensor(19.5072), tensor(19.4622), tensor(18.9521), tensor(18.6183), tensor(18.4549), tensor(18.1767), tensor(16.9969), tensor(16.3657), tensor(17.5590), tensor(18.2751), tensor(18.7698), tensor(18.6369), tensor(19.1837), tensor(19.4913), tensor(19.1978), tensor(18.3439), tensor(16.2896), tensor(19.2105), tensor(18.9137), tensor(18.5078), tensor(19.2497), tensor(18.0722), tensor(17.7736), tensor(19.0498), tensor(18.2858), tensor(18.5505), tensor(18.2629), tensor(17.7679), tensor(18.8875), tensor(19.1686), tensor(17.9744), tensor(18.4645), tensor(19.1880), tensor(17.1785), tensor(19.0714), tensor(20.0467), tensor(19.6531), tensor(17.6061), tensor(18.7277), tensor(18.0048), tensor(19.8096), tensor(18.0334), tensor(19.4394), tensor(18.8188), tensor(17.3270), tensor(18.9399), tensor(18.6578), tensor(18.5586), tensor(18.0344), tensor(17.5805), tensor(18.7055), tensor(18.4997), tensor(17.6937), tensor(18.6879), tensor(16.4723), tensor(18.1980), tensor(18.4757), tensor(18.4517), tensor(17.4676), tensor(17.0324), tensor(19.5186), tensor(18.6344), tensor(18.0288), tensor(18.1127), tensor(18.2029), tensor(18.8603), tensor(19.6593), tensor(16.9604), tensor(18.0242), tensor(16.9264), tensor(19.1215), tensor(15.3452), tensor(19.5143), tensor(18.8715), tensor(19.0426), tensor(18.1615), tensor(18.9730), tensor(17.4788), tensor(19.0171), tensor(17.5345), tensor(17.5922), tensor(19.2731), tensor(17.5122), tensor(19.2313), tensor(18.9755), tensor(17.1802), tensor(19.2188), tensor(18.5946), tensor(18.3661), tensor(19.4749), tensor(19.4623), tensor(18.8479), tensor(18.1962), tensor(17.2014), tensor(18.7758), tensor(18.4626), tensor(18.2084), tensor(18.2081), tensor(17.4931), tensor(18.3176), tensor(19.2710), tensor(16.7000), tensor(16.0778), tensor(19.9661), tensor(17.4111), tensor(17.1182), tensor(18.4445), tensor(18.6598), tensor(18.8970), tensor(17.8695), tensor(17.7450), tensor(18.8554), tensor(17.5850), tensor(17.7666), tensor(18.1932), tensor(17.3866), tensor(19.8638), tensor(17.1727), tensor(17.1960), tensor(18.3614), tensor(17.7816), tensor(17.2605), tensor(18.8002), tensor(18.6879), tensor(19.1866), tensor(19.1603), tensor(19.1783), tensor(18.3705), tensor(17.7206), tensor(18.8533), tensor(17.7045), tensor(17.9499), tensor(18.5938), tensor(18.8599), tensor(18.7277), tensor(17.2488), tensor(19.3042), tensor(19.1544), tensor(18.1653), tensor(17.8188), tensor(18.8647), tensor(16.4993), tensor(18.1230), tensor(19.7421), tensor(20.0098), tensor(19.2414), tensor(18.2889), tensor(18.9202), tensor(18.0009), tensor(17.1247), tensor(18.0450), tensor(17.6830), tensor(15.7632), tensor(17.5885), tensor(18.7492), tensor(18.5257), tensor(17.1382), tensor(18.9539), tensor(16.8081), tensor(19.0066), tensor(18.0739), tensor(16.7641), tensor(16.8931), tensor(18.4416), tensor(17.9874), tensor(19.3418), tensor(19.6339), tensor(18.4118), tensor(17.4202), tensor(19.1037), tensor(17.3080), tensor(18.5511), tensor(17.2301), tensor(17.7541), tensor(18.5168), tensor(18.4568), tensor(18.8028), tensor(19.0287), tensor(18.4733), tensor(19.1988), tensor(19.1673), tensor(18.2506), tensor(17.8089), tensor(18.7511), tensor(16.8578), tensor(16.9281), tensor(17.8671), tensor(16.8839), tensor(18.1746), tensor(18.2497), tensor(17.4489), tensor(18.0779), tensor(18.2761), tensor(17.7960), tensor(16.5653), tensor(14.9341), tensor(18.6993), tensor(17.7917), tensor(18.8273), tensor(18.3879), tensor(17.7320), tensor(19.0733), tensor(18.4419), tensor(18.0739), tensor(18.3135), tensor(18.5416), tensor(18.1109), tensor(18.5054), tensor(17.1074), tensor(18.1129), tensor(16.3384), tensor(19.1652), tensor(18.6807), tensor(17.0877), tensor(18.4991), tensor(17.9810), tensor(19.5482), tensor(17.5069), tensor(18.9355), tensor(18.1962), tensor(19.1587), tensor(18.7661), tensor(19.2068), tensor(18.5650), tensor(18.9801), tensor(18.8387), tensor(18.4418), tensor(17.7976), tensor(17.9342), tensor(17.7375), tensor(16.4824), tensor(16.9071), tensor(19.2031)]\n",
      "[tensor(19.4406), tensor(18.4360), tensor(19.3618), tensor(17.8598), tensor(18.9024), tensor(18.1766), tensor(19.5036), tensor(17.8387), tensor(17.5989), tensor(18.0780), tensor(18.1534), tensor(17.6102), tensor(19.1630), tensor(16.5459), tensor(18.4380), tensor(19.1982), tensor(17.8587), tensor(17.8579), tensor(18.2196), tensor(18.7324), tensor(17.1352), tensor(18.0492), tensor(18.5776), tensor(17.7276), tensor(18.8504), tensor(18.5080), tensor(16.9928), tensor(17.0407), tensor(19.6952), tensor(17.4288), tensor(17.2900), tensor(17.4285), tensor(18.0059), tensor(18.8600), tensor(17.6256), tensor(18.9638), tensor(17.0024), tensor(18.6736), tensor(16.7323), tensor(17.1294), tensor(19.0426), tensor(17.4640), tensor(18.6463), tensor(17.3394), tensor(17.9125), tensor(19.3974), tensor(18.5379), tensor(19.5830), tensor(18.2832), tensor(18.3134), tensor(16.4558), tensor(18.4111), tensor(19.0550), tensor(17.0958), tensor(19.3097), tensor(18.8220), tensor(19.0889), tensor(18.8972), tensor(17.5980), tensor(18.6677), tensor(19.2793), tensor(19.1682), tensor(17.4386), tensor(17.6866), tensor(19.0052), tensor(18.6928), tensor(19.4793), tensor(18.6351), tensor(17.7886), tensor(18.2951), tensor(18.3077), tensor(19.6308), tensor(19.4393), tensor(17.8232), tensor(17.8332), tensor(18.1992), tensor(18.6892), tensor(17.9116), tensor(18.6323), tensor(18.8660), tensor(18.8347), tensor(19.9434), tensor(19.4438), tensor(17.4483), tensor(18.0982), tensor(18.6423), tensor(18.8235), tensor(17.7716), tensor(18.5202), tensor(19.5378), tensor(17.7456), tensor(18.5013), tensor(18.7927), tensor(20.1064), tensor(19.0968), tensor(19.5762), tensor(19.5935), tensor(18.8714), tensor(17.9828), tensor(17.3961), tensor(18.4284), tensor(16.5626), tensor(18.8747), tensor(18.7911), tensor(18.2468), tensor(17.8507), tensor(17.8058), tensor(19.4415), tensor(17.2472), tensor(17.2299), tensor(19.4050), tensor(18.1927), tensor(18.2729), tensor(19.1244), tensor(20.2740), tensor(17.8505), tensor(18.4906), tensor(18.5001), tensor(18.1664), tensor(16.3557), tensor(16.6467), tensor(16.4031), tensor(18.4382), tensor(18.4121), tensor(18.3269), tensor(18.8436), tensor(18.4329), tensor(18.0576), tensor(17.5573), tensor(18.2002), tensor(19.2036), tensor(19.2051), tensor(19.2136), tensor(16.9431), tensor(17.3514), tensor(18.9649), tensor(19.3555), tensor(16.3081), tensor(18.9409), tensor(19.6319), tensor(19.1855), tensor(19.9070), tensor(18.3416), tensor(18.0066), tensor(18.3072), tensor(17.5830), tensor(19.0723), tensor(19.4899), tensor(16.8405), tensor(17.9032), tensor(17.6596), tensor(17.4827), tensor(18.0641), tensor(16.7971), tensor(18.4760), tensor(18.3446), tensor(18.9199), tensor(18.5887), tensor(18.7337), tensor(18.3434), tensor(17.6647), tensor(17.6079), tensor(17.6971), tensor(18.2162), tensor(18.2323), tensor(17.8831), tensor(19.2611), tensor(16.8803), tensor(18.1422), tensor(17.8554), tensor(18.6308), tensor(17.9354), tensor(18.7500), tensor(16.5139), tensor(19.3223), tensor(18.4486), tensor(18.4439), tensor(19.5618), tensor(19.0572), tensor(16.9321), tensor(17.4026), tensor(19.8573), tensor(18.0312), tensor(18.5306), tensor(19.5050), tensor(17.4376), tensor(17.8941), tensor(19.4833), tensor(19.1124), tensor(19.1003), tensor(18.2085), tensor(17.9916), tensor(17.3158), tensor(19.0309), tensor(18.9365), tensor(18.9416), tensor(17.8605), tensor(19.3084), tensor(18.7354), tensor(18.3766), tensor(19.5605), tensor(19.0047), tensor(18.6736), tensor(19.6730), tensor(16.2089), tensor(19.8492), tensor(18.7031), tensor(19.4131), tensor(19.3262), tensor(16.4193), tensor(18.9931), tensor(16.7010), tensor(19.4755), tensor(17.6539), tensor(17.2018), tensor(17.8778), tensor(18.7094), tensor(17.5748), tensor(18.5358), tensor(18.9058), tensor(18.8226), tensor(20.1850), tensor(18.6294), tensor(18.1657), tensor(17.4734), tensor(19.0999), tensor(20.2039), tensor(18.7854), tensor(17.9407), tensor(18.4259), tensor(18.0756), tensor(18.1851), tensor(18.7182), tensor(16.2875), tensor(18.3755), tensor(18.9325), tensor(20.5133), tensor(18.1821), tensor(18.2145), tensor(18.8244), tensor(18.8565), tensor(17.9532), tensor(18.8267), tensor(18.9906), tensor(19.3701), tensor(17.1608), tensor(19.5315), tensor(18.5202), tensor(18.5461), tensor(19.5582), tensor(17.6737), tensor(19.8997), tensor(16.4692), tensor(17.8030), tensor(17.4988), tensor(18.6073), tensor(16.7280), tensor(20.4215), tensor(18.2184), tensor(19.7590), tensor(19.5593), tensor(20.4729), tensor(19.6955), tensor(18.7798), tensor(17.2596), tensor(20.0304), tensor(19.3420), tensor(19.0530), tensor(16.9008), tensor(19.5230), tensor(17.7101), tensor(18.5397), tensor(16.9105), tensor(18.5965), tensor(19.0510), tensor(18.7304), tensor(19.3394), tensor(17.3905), tensor(16.6775), tensor(16.6632), tensor(17.9431), tensor(18.3788), tensor(18.8403), tensor(19.1215), tensor(18.9359), tensor(17.9140), tensor(16.8869), tensor(19.1952), tensor(17.0298), tensor(19.2833), tensor(20.1272), tensor(19.1801), tensor(19.0992), tensor(18.1592), tensor(19.9111), tensor(19.5494), tensor(18.9196), tensor(19.3903), tensor(19.0865), tensor(19.4877), tensor(19.3931), tensor(18.6313), tensor(16.6210), tensor(17.6786), tensor(18.6158), tensor(18.4423), tensor(18.6263), tensor(18.4937), tensor(18.8940), tensor(18.6423), tensor(17.7029), tensor(17.0849), tensor(19.0985), tensor(18.2462), tensor(18.2685), tensor(16.9666), tensor(19.8415), tensor(17.0328), tensor(18.6613), tensor(17.1227), tensor(18.0027), tensor(19.3565), tensor(17.5201), tensor(17.6672), tensor(17.7188), tensor(19.0755), tensor(19.1613), tensor(19.0419), tensor(19.1056), tensor(19.3355), tensor(17.9286), tensor(16.9599), tensor(18.0204), tensor(18.3614), tensor(19.3772), tensor(18.6080), tensor(17.9827), tensor(16.9390), tensor(17.5983), tensor(18.5233), tensor(19.6457), tensor(19.4760), tensor(18.1700), tensor(17.5005), tensor(17.8783), tensor(18.5843), tensor(19.0773), tensor(18.8153), tensor(17.4192), tensor(18.9604), tensor(18.8650), tensor(19.3603), tensor(18.5147), tensor(19.2202), tensor(19.0883), tensor(19.6042), tensor(17.2985), tensor(18.5694), tensor(20.3519), tensor(18.3448), tensor(16.6489), tensor(17.7938), tensor(19.0215), tensor(17.3789), tensor(18.4050), tensor(18.8145), tensor(18.3576), tensor(19.5058), tensor(17.7940), tensor(18.7932), tensor(19.0648), tensor(19.7326), tensor(19.3768), tensor(18.6918), tensor(18.9269), tensor(18.0497), tensor(17.7341), tensor(19.3309), tensor(18.9343), tensor(19.3998), tensor(19.1187), tensor(19.0868), tensor(20.1432), tensor(19.7322), tensor(20.3670), tensor(18.7005), tensor(19.1597), tensor(18.4131), tensor(17.9473), tensor(18.6196), tensor(18.5312), tensor(18.1109), tensor(19.4325), tensor(18.2304), tensor(18.6677), tensor(16.9753), tensor(18.6183), tensor(19.1797), tensor(16.3903), tensor(17.9143), tensor(20.1900), tensor(19.3065), tensor(18.8217), tensor(19.1534), tensor(17.8142), tensor(19.1292), tensor(18.1691), tensor(17.9709), tensor(17.3843), tensor(17.8314), tensor(18.7153), tensor(18.6755), tensor(17.6705), tensor(19.7500), tensor(19.6791), tensor(19.3219), tensor(18.8829), tensor(20.0216), tensor(19.3231), tensor(20.2444), tensor(18.8574), tensor(17.2583), tensor(16.5727), tensor(19.5401), tensor(19.6994), tensor(18.1630), tensor(19.2731), tensor(19.2092), tensor(19.0819), tensor(19.6285), tensor(17.6863), tensor(16.9603), tensor(18.5961), tensor(17.7433), tensor(16.6776), tensor(19.6598), tensor(18.2175), tensor(17.8499), tensor(17.6624)]\n"
     ]
    }
   ],
   "source": [
    "### Task 1\n",
    "### YOUR CODE\n",
    "from sklearn.covariance import LedoitWolf\n",
    "import numpy as np\n",
    "\n",
    "# Tumor\n",
    "cov_tumor = LedoitWolf().fit(train_features[train_labels==0]).covariance_\n",
    "cov_tumor = np.linalg.inv(cov_tumor)\n",
    "mean_tumor = torch.mean(train_features[train_labels==0], dim=0, keepdim=False)\n",
    "mahalanobis_distance_tumor = []\n",
    "for i in range(train_features[train_labels==0].shape[0]):\n",
    "    mahalanobis_distance_tumor.append(np.sqrt((train_features[train_labels==0][i]-mean_tumor).T @ cov_tumor @ (train_features[train_labels==0][i]-mean_tumor)))\n",
    "\n",
    "# Stroma\n",
    "cov_stroma = LedoitWolf().fit(train_features[train_labels==1]).covariance_\n",
    "cov_stroma = np.linalg.inv(cov_stroma)\n",
    "mean_stroma = torch.mean(train_features[train_labels==1], dim=0, keepdim=False)\n",
    "mahalanobis_distance_stroma = []\n",
    "for i in range(train_features[train_labels==1].shape[0]):\n",
    "    mahalanobis_distance_stroma.append(np.sqrt((train_features[train_labels==1][i]-mean_stroma).T @ cov_stroma @ (train_features[train_labels==1][i]-mean_stroma)))\n",
    "\n",
    "\n",
    "pred_labels = mahalanobis_distance_stroma < mahalanobis_distance_tumor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 points)** Compute the accuracy of your predictions with the test labels (```test_labels```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2\n",
    "\n",
    "# Accuracy = (number of correct predictions) / (total number of predictions) or (TP + TN) / (TP + TN + FP + FN)\n",
    "accuracy = (pred_labels==test_labels).sum()/len(test_labels)\n",
    "\n",
    "print(\"Accuracy: {:.2f}\".format(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Out-of-Distribution detection with Mahalanobis distance (3 points)\n",
    "\n",
    "You will note that the test you run above is not really realistic. Like the training set, it contains only the TUMOR and STROMA tissue types. Nevertheless, at test time, the other tissues (Label -1) are also present and cannot be filtered by hand. Moreover, they cannot be recognized by the model as they are out of the training distribution (It is the consequence of the laziness of the annotators ;)). For this reason, it is essential to filter them out. This task is called Out-of-Distribution (OoD) detection. \n",
    "\n",
    "A simple way to do OoD detection is to compute for every test example an OoD-ness score which should be low for In-Distribution (ID) examples and high for OoDs. Then we define a threshold from which every example with an OoD-ness lying above is discarded, and those lying below are forwarded to the model for prediction. An example of OoD-ness score is the minimum Mahalanobis distance.\n",
    "\n",
    "Run the cell below to load a new test set containing OoD examples. It has 186 ID and 558 OoD examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_classname_w_ood = {0 : \"TUMOR\", 1 : \"STROMA\", -1 : \"OoD\"}\n",
    "\n",
    "# Test features and labels with OoD tissues\n",
    "test_features_w_ood = torch.load(os.path.join(data_base_path, data_folder, \"part1/k16_test2_features.pth\"))\n",
    "test_labels_w_ood = torch.load(os.path.join(data_base_path, data_folder,\"part1/k16_test2_labels.pth\"))\n",
    "\n",
    "test_features_w_ood.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (0.5 point)** Why do you think the minimum Mahalanobis distance is a good OoD-ness score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 point)** Compute the minimum Mahalanobis distance for every test examples in ```test_features_w_ood``` with respect to the training features (```train_features```)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (0.5 point)** Plot a histogram to show the difference between the Mahalanobis distance of TUMOR, STROMA and OoD tissue types and comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 3\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Find a threshold on the Mahalanobis distance such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 4\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (0.5 point)** Assign prediction -1 to filtered out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```). Is it satisfactory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 5\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Out-of-distribution detection with k-NN classifier (6 points)\n",
    "\n",
    "The visual foundation models are known to be very good k-NN classifiers. It motivates us to implement a k-NN classifier to recognize TUMOR and STROMA. Moreover, k-NN distance is a good OoD-ness score and suits our task.\n",
    "\n",
    "**Task 1 (2 points)** Based on the training features (```train_features```) and training labels (```train_labels```), classify the test features (```test_features```) using a k-NN classifier. Then report the accuracy of your predictions with the test labels (```test_labels```).\n",
    "\n",
    "*Note:* The choice of `k` is up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 1\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (2 points)** Perform OoD detection on the test features (```test_features_w_ood```) using a k-NN distance based OoD-ness score. Find a threshold on your OoD-ness score such that 95% of the OoD examples are filtered out. How much TUMOR and STROMA have also been filtered out? Finally, assign prediction -1 to filter out examples and compute the average class-wise accuracy of your prediction with test labels (```test_labels_w_ood```).\n",
    "\n",
    "*Note:* The OoD-ness is based on the distance to the k-nearest neighbors. The formulation is up to you. You have to justify your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 2\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (1 point)** Is k-NN better than Mahalanobis distance ? Make an hypothesis for the reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (1 point)** Do you think we can suggest the approach presented in this exercise to compute TUMOR/STROMA ratio automatically ? Justify your thoughs. If not, suggest at least two ideas to improve it.\n",
    "\n",
    "*Note:* Annotating all the training dataset is not an option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2 (12 points)\n",
    "In this part, we aim to classify cervical cells resulting from Pap smear tests. To that end we'll be using a publicly available cell dataset: Sipakmed (https://www.cs.uoi.gr/~marina/sipakmed.html). The dataset is composed of 4049 images of isolated cells cropped from 966 cluster cell images of Pap smear slides. Each cell in the dataset has been categorized in either of the following categories: \n",
    "\n",
    "    - Superficial-Intermediate.\n",
    "    - Parabasal.\n",
    "    - Koilocytotic.\n",
    "    - Dysketarotic.\n",
    "    - Metaplastic.\n",
    "Your objective is to implement a classifier to automate the cell classification process. To ease your work we provide you with pre-computed embeddings for each images (`lab-03-data/part2/sipakmed_clean_embeddings.pth`). The embeddings are obtained from a pre-trained ResNet-50 (https://arxiv.org/pdf/1512.03385.pdf) and the corresponding images are also provided (`lab-03-data/part2/sipakmed_clean`). Note that you are free to discard the provided embeddings and work directy with the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dataset (4 points)\n",
    "Your first task is prepare the dataset such that it can be used to train your model. For that purpose we prepared the skeleton of the class `Sipakmed` that inherits from the class `Dataset` of PyTorch. Read the documentation (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) and complete the missing parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch import nn\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the features\n",
    "features_path = '../data/lab-03-data2023/part2/sipakmed_clean_embeddings.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sipakmed(Dataset):\n",
    "    phase_dict = {\n",
    "            'train': {'start': 0.0, 'stop': 0.5},\n",
    "            'val': {'start': 0.5, 'stop': 0.75},\n",
    "            'test': {'start': 0.75, 'stop': 1.0}\n",
    "    }\n",
    "    label_dict = {\n",
    "        'im_Superficial-Intermediate': 0,\n",
    "        'im_Parabasal': 1, \n",
    "        'im_Metaplastic': 2,\n",
    "        'im_Koilocytotic': 3,\n",
    "        'im_Dyskeratotic': 4\n",
    "    }\n",
    "    \n",
    "    def __init__(self, features_path, phase):\n",
    "\n",
    "        super(Sipakmed, self).__init__()\n",
    "        # Store class attributes\n",
    "        self.phase = phase\n",
    "        \n",
    "        # Collect the dataimport torch\n",
    "        import torch.nn.functional as F\n",
    "        import numpy as np\n",
    "        self.raw_data = torch.load(features_path)\n",
    "        self.features, self.labels, self.paths = self.collect_data()\n",
    "        \n",
    "    def collect_data(self):\n",
    "        # Iterate over the dirs/classes\n",
    "        features, labels, paths = [], [], []\n",
    "        for dir_name, dir_dict in self.raw_data.items():\n",
    "            # Get the paths and embeddings\n",
    "            dir_paths, dir_embeddings = list(zip(*[(k, v) for k, v in dir_dict.items()]))\n",
    "            \n",
    "            # Split\n",
    "            n = len(dir_paths)\n",
    "            np.random.seed(42)\n",
    "            permutations = np.random.permutation(n)\n",
    "            dir_paths = np.array(dir_paths)[permutations]\n",
    "            dir_embeddings = torch.stack(dir_embeddings)[permutations]\n",
    "            n_start = int(n * self.phase_dict[self.phase]['start'])\n",
    "            n_stop = int(n * self.phase_dict[self.phase]['stop'])\n",
    "            dir_embeddings = dir_embeddings[n_start: n_stop]\n",
    "            dir_paths = dir_paths[n_start: n_stop]\n",
    "    \n",
    "            # Store\n",
    "            features.append(dir_embeddings)\n",
    "            paths.append(dir_paths)\n",
    "            dir_labels = torch.tensor([self.label_dict[p.split('/')[-2]] for p in dir_paths])\n",
    "            labels.append(dir_labels)\n",
    "            \n",
    "        # Merge\n",
    "        features = torch.cat(features)\n",
    "        labels = torch.cat(labels)\n",
    "        paths = np.concatenate(paths)\n",
    "        return features, labels, paths\n",
    "            \n",
    "        \n",
    "    def __len__(self,):\n",
    "        \"\"\"\n",
    "        Returns the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns the embedding, label, and image path of queried index.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE\n",
    "        return embedding, label, path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the implementation of `Sipakmed` completed, create 3 instances of the class (train/val/test) with the corresponding `phase` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the datasets\n",
    "train_dataset = ### YOUR CODE\n",
    "val_dataset = ### YOUR CODE\n",
    "test_dataset = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that your datasets are ready, use the class `DataLoader` from PyTorch to let it handle efficiently the batching, shuffling, etc. of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the data loaders\n",
    "train_loader = ### YOUR CODE\n",
    "val_loader = ### YOUR CODE\n",
    "test_loader = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get to know your data. Plot a few example images for each class of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some training example\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Training (4 points)\n",
    "In this part your objective is to implement the required tools to train your model. The first thing you'll need is a a model which takes as input the pre-computed features and returns the corresponding class probabilities/logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the model\n",
    "embedding_dim = train_dataset.features.shape[1]\n",
    "model = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will keep track of your model's parameters, gradients, etc (https://pytorch.org/docs/stable/optim.html). It is responsible to update your model's parameters after each forward pass using the backpropagation algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the optimizer\n",
    "optimizer = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the loss\n",
    "criterion = ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a function that takes as input the model's output and the corresponding labels and returns the perçentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    \"\"\"\n",
    "    Computes the accuracy of predictions based on the model outputs (NxK: N samples, K classes) \n",
    "    and the labels (N: N samples).\n",
    "    \"\"\"\n",
    "    ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `train` that forwards the complete training set through your model (= 1 epoch) and updates its parameters after each forward pass. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths \n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Reset the gradients\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Backpropagate\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Update the parameters\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a funtion `validate` that forwards the complete validation or test set through your model and evaluates its predictions. To keep track of the training process make sure to at least return the accuracy of the model and the average loss it incurred through the current epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def validate(model, criterion, loader):\n",
    "    # Set the model in train mode\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Iterate over the batches\n",
    "    full_outputs = []\n",
    "    full_labels = []\n",
    "    full_paths = []\n",
    "    losses = []\n",
    "    for batch in loader:\n",
    "        # Get the embeddings, labels and paths\n",
    "        ### YOUR CODE\n",
    "        \n",
    "        # Feed the embeddings to the model\n",
    "        ### YOUR CODE\n",
    "\n",
    "        # Compute cross entropy loss\n",
    "        l### YOUR CODE\n",
    "        \n",
    "        # Store the outputs, labels and loss\n",
    "        ### YOUR CODE\n",
    "    \n",
    "    # Concat\n",
    "    full_outputs = torch.cat(full_outputs).cpu()\n",
    "    full_labels = torch.cat(full_labels).cpu()\n",
    "    losses = torch.stack(losses).mean().cpu()\n",
    "    full_paths = np.concatenate(full_paths)\n",
    "    \n",
    "    # Compute the accuracy\n",
    "    ### YOUR CODE\n",
    "    return acc, full_outputs, full_labels, losses, full_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now be able to train you model. Alternate between training and validation steps to find and save the best model (best accuracy on the validation set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main loop\n",
    "epochs = ### YOUR CODE\n",
    "best_acc = ### YOUR CODE\n",
    "model_savepath = '../data'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train\n",
    "    ### YOUR CODE\n",
    "\n",
    "    # Evaluate\n",
    "    ### YOUR CODE\n",
    "    \n",
    "    # Save the model\n",
    "    if val_acc > best_acc:\n",
    "        ### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Evaluation (4 points)\n",
    "Re-load the best model and evaluate its predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-load the best model\n",
    "### YOUR CODE\n",
    "\n",
    "# Evaluate\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool to analyze your model's performance on the different classes is the confusion matrix (https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html). Computes its entries for your model and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrix\n",
    "### YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively it can be useful to plot the problematic samples as well as the predicted and ground truth classes. Can you do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the misclassified samples\n",
    "### YOUR CODE\n",
    "\n",
    "# Plot the misclassified samples\n",
    "### YOUR CODE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iapr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c4f8c0b90837f8d1af7a1beb73ba126fe15da9bf616d135eba9f0af5b82d9de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
